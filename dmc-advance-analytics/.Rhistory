# Antes de nada, limpiamos el workspace, por si hubiera algun dataset o informacion cargada
rm(list = ls())
# Cambiar el directorio de trabajo
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# Limpiamos la consola
cat("\014")
# Vamos a cargar las librerías necesarias
library(pdftools)
library(dplyr)
library(stopwords)
library(tidytext)
library(stringi)
library(stringr)
library(ggplot2)
library(scales)
library(tidyr)
library(widyr)
library(ggraph)
library(igraph)
library(quanteda)
library(topicmodels)
library(cvTools) #  Para el cálculo de topics
library(lubridate) # Para el redondeo de fechas
library(SnowballC) #  Para hacer stemming
##########################################################################
# 1. Leemos las opiniones para empezar con el ejercicio
##########################################################################
opiniones<-read.csv("data/opinionesPrado.csv")
# opiniones<-read.csv("Datos/opinionesGuggenheim.csv")
opiniones
# Mejoramos un poco la calidad de los datos
opiniones=opiniones[,c(3:4)]
colnames(opiniones)[1]<-"review_body"
str(opiniones)
opiniones$review_body=as.character(opiniones$review_body)
opiniones$review_date=as.Date(opiniones$review_date)
str(opiniones)
opiniones %>%
dplyr::count(Week = round_date(review_date, "week")) %>% # por semana
ggplot(aes(Week, n)) +
geom_line() +
ggtitle('El numero de comentarios por semana')
# 1.1. Algunos análisis básicos
df <- tibble::rowid_to_column(opiniones, "ID") # añadiendo un ID
df <- df %>%
mutate(review_date = as.POSIXct(review_date, origin = "1970-01-01"),
month = round_date(review_date, "month"))
?rowid_to_column
# Nos creamos un lexicon de stopwords en español a medida
custom_stop_words <- stopwords("es")
custom_stop_words <- append(custom_stop_words, c("má","museo","más","mã",""," ")) # adicionando temas fijos
custom_stop_words <- as.data.frame(custom_stop_words)
names(custom_stop_words) <- "word"
custom_stop_words$word <- as.character(custom_stop_words$word)
# Sin quitar stop_words
distintos <- df %>%
distinct(review_body, .keep_all = TRUE) # verificar tamaño
df.1 <- df[1:10,]
dim(df)
length(custom_stop_words$word)
review_words <- df %>%
distinct(review_body, .keep_all = TRUE) %>% # si es FALSE no ejecuta la eliminacion de repetidos
unnest_tokens(word, review_body, drop = FALSE) %>% # crea una columnas con los tokens de cada opinion
distinct(ID, word, .keep_all = TRUE) %>% # elimina repetidos
filter(str_detect(word, "[^\\d]")) %>% # Expresion regular para negar la extraccion de letras
group_by(word) %>%
mutate(word_total = n()) %>%
ungroup()
# Quitando stopwords
review_words <- df %>%
distinct(review_body, .keep_all = TRUE) %>%
unnest_tokens(word, review_body, drop = FALSE) %>%
distinct(ID, word, .keep_all = TRUE) %>%
anti_join(custom_stop_words) %>%   # quito los stop_word
filter(str_detect(word, "[^\\d]")) %>%
group_by(word) %>%
mutate(word_total = n()) %>%
ungroup()
word_counts <- review_words %>%
count(word, sort = TRUE)
# Formateamos un poco la gráfica
word_counts %>%
head(25) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(fill = "blue") +
scale_y_continuous(labels = comma_format()) +
coord_flip() +
labs(title = "Palabras + usadas Museo del Prado-TripAdvisor(2005-18)",
subtitle = "29061 opiniones; stopwords retiradas", x = "Palabra",
y = "Número de veces usada")
# Y ahora haciendo stemming:
word_counts %>%
head(25) %>%
mutate(word = wordStem(word)) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(fill = "blue") +
scale_y_continuous(labels = comma_format()) +
coord_flip() +
labs(title = "Palabras más usadas para describir Museo del Prado en TripAdvisor (2005-2018)",
subtitle = "29061 opiniones; stopwords retiradas y stemming", x = "Palabra",
y = "Número de veces usada")
# 1.2. Bigramas
# A veces nos interesa entender la relación entre palabras en una opinión.
review_bigrams <- df %>%
unnest_tokens(bigram, review_body, token = "ngrams", n = 2)
bigrams_separated <- review_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% custom_stop_words$word) %>%
filter(!word2 %in% custom_stop_words$word)
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
bigrams_united %>%
count(bigram, sort = TRUE)
# Podemos visualizarlo también
review_subject <- df %>%
unnest_tokens(word, review_body) %>%
anti_join(custom_stop_words)
my_stopwords <- data_frame(word = c(as.character(1:10)))
review_subject <- review_subject %>%
anti_join(my_stopwords)
title_word_pairs <- review_subject %>%
pairwise_count(word, ID, sort = TRUE, upper = FALSE)
set.seed(1234)
title_word_pairs %>%
filter(n >= 150) %>%
graph_from_data_frame() %>%
ggraph(layout = "fr") +
geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
geom_node_point(size = 5) +
geom_node_text(aes(label = name), repel = TRUE,
point.padding = unit(0.2, "lines")) +
ggtitle('Red de palabras Museo del Prado - TripAdvisor')
corpus <- corpus(opiniones$review_body)
cdfm <- dfm(corpus, remove=c(stopwords("spanish"),
"t.co", "https", "museo","prado",
"rt","amp", "españa","españoles",
"t","i", "http", "t.c", "can"),
verbose=TRUE, remove_punct=TRUE, remove_numbers=TRUE)
cdfm <- dfm_trim(cdfm, min_docfreq = 2, verbose=TRUE)
# Ahora lo exportamos a un formato para procesar los Topic Models.
dtm <- convert(cdfm, to="topicmodels")
## Seleccionando el número de topics óptimo
# Estimamos el LDA con el número óptimo de topics que nos haya salido
K <- 20
lda <- LDA(dtm, k = K, method = "Gibbs",
control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
## Seleccionando el número de topics óptimo
# Estimamos el LDA con el número óptimo de topics que nos haya salido
K <- 20
lda <- LDA(dtm, k = K, method = "Gibbs",
control = list(verbose=25L, seed = 123, burnin = 100, iter = 500))
